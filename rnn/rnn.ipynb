{"cells":[{"cell_type":"markdown","metadata":{"id":"DexpUSlMjnY2"},"source":["# Recurrent Neural Network and Multi-Head Attention (MHA)\n","\n","In this task, you will implement a conventional RNN cell, a GRU, and an MHA to understand these models. Then you will configurate GRU in special ways such that it either recovers a conventional RNN or keeps its memory in long term. NOTE: you should not change the provided function interfaces and test cases. "]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":34},"id":"KrvHEyNOjnY6","executionInfo":{"status":"ok","timestamp":1669926284662,"user_tz":300,"elapsed":4365,"user":{"displayName":"Irene N. Chang","userId":"18402260844163642514"}},"outputId":"11b216c6-25a8-4ea8-b0aa-1064eaa5d718"},"outputs":[{"output_type":"display_data","data":{"application/javascript":["IPython.notebook.set_autosave_interval(180000)"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Autosaving every 180 seconds\n"]}],"source":["# As usual, a bit of setup\n","import time\n","import numpy as np\n","import torch\n","import matplotlib.pyplot as plt\n","\n","\n","%matplotlib inline\n","plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n","plt.rcParams['image.interpolation'] = 'nearest'\n","plt.rcParams['image.cmap'] = 'gray'\n","\n","# for auto-reloading external modules\n","# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n","%load_ext autoreload\n","%autoreload 2\n","%autosave 180\n","\n","def rel_error(x, y):\n","    \"\"\" returns relative error \"\"\"\n","    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))\n"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","\n","import sys\n","sys.path.append(\"/content/drive/MyDrive/cs137assignments/assignment4\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DumtYcJTjwZe","executionInfo":{"status":"ok","timestamp":1669926308198,"user_tz":300,"elapsed":20565,"user":{"displayName":"Irene N. Chang","userId":"18402260844163642514"}},"outputId":"26c81dd6-4ccf-409d-fa92-c58c2b10c627"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","metadata":{"id":"PmsMJr3OjnY9"},"source":["## Recurrent Neural Networks\n","\n","In this task, you will need to implement forward calculation of recurrent neural networks. Let's first initialize a problem for RNNs."]},{"cell_type":"code","execution_count":15,"metadata":{"id":"JASyEkexjnY-","executionInfo":{"status":"ok","timestamp":1669927843795,"user_tz":300,"elapsed":936,"user":{"displayName":"Irene N. Chang","userId":"18402260844163642514"}}},"outputs":[],"source":["import torch.nn as nn\n","## Setup an example. Provide sizes and the input data. \n","\n","# set sizes \n","time_steps = 12\n","batch_size = 4\n","input_size = 3\n","hidden_size = 2\n","\n","# create input data with shape [batch_size, time_steps, num_features]\n","np.random.seed(137)\n","input_data = torch.randn(batch_size, time_steps, input_size, dtype = torch.float32)\n","## Create RNN layers\n","\n","# initialize a state of zero for both RNN and GRU\n","# 'state' is a tensor of shape [batch_size, hidden_size]\n","initial_state = torch.randn(batch_size, hidden_size, dtype = torch.float32).unsqueeze(0)"]},{"cell_type":"markdown","metadata":{"id":"Np6U6SqpjnY_"},"source":["### Implement an RNN and a GRU with PyTorch"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"cHzK4DQzjnZA","executionInfo":{"status":"ok","timestamp":1669927846500,"user_tz":300,"elapsed":110,"user":{"displayName":"Irene N. Chang","userId":"18402260844163642514"}}},"outputs":[],"source":["# create an RNN with only one layer from torch\n","t_rnn = nn.RNN(input_size, hidden_size, num_layers = 1, batch_first = True)\n","\n","# 'outputs' is a tensor of shape [batch_size, time_steps, hidden_size]\n","# RNN cell outputs the hidden state directly, so the output at each step is the hidden state at that step\n","# final_state is the last state of the sequence. final_state == outputs[:, -1, :]\n","\n","# create a GRU RNN\n","t_gru = nn.GRU(input_size, hidden_size, num_layers = 1, batch_first = True)\n","\n","with torch.no_grad():\n","    t_rnn_outputs, t_rnn_final_state = t_rnn(input_data, initial_state)\n","    # 'outputs' and `final_state` are the same for a GRU.\n","    t_gru_outputs, t_gru_final_state = t_gru(input_data, initial_state)"]},{"cell_type":"markdown","metadata":{"id":"a9hUw8EwjnZB"},"source":["### Read out parameters from RNN and GRU cells\n","\n","**Q1 (0 points)** Understanding `RNN` and `GRU` parameters\n","\n","Please read the code and documentation of `get_rnn_params` and `get_gru_params` to see how to read out parameters from these to models. You will need to use these parameters in your own implementations. NO implementation is needed here.\n"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"DjfmWrZljnZC","executionInfo":{"status":"ok","timestamp":1669927848572,"user_tz":300,"elapsed":130,"user":{"displayName":"Irene N. Chang","userId":"18402260844163642514"}}},"outputs":[],"source":["from rnn_param_helper import get_rnn_params, get_gru_params\n","\n","wt_h, wt_x, bias = get_rnn_params(t_rnn)\n","\n","# NOTE: please check the documentation of `torch.nn.GRU` and the implementation of `get_gru_params` to \n","# understand the three returning arguments.\n","\n","linear_trans_r, linear_trans_z, linear_trans_n = get_gru_params(t_gru)"]},{"cell_type":"markdown","metadata":{"id":"-plhgbAnjnZD"},"source":["### Numpy Implementation\n","**Q2 (3 points)** Please implement your own simple RNN. \n","\n","Your implementation needs to match the tensorflow calculation.\n","\n","**Q3 (5 points)** Please implement your own GRU. \n","\n","Your implementation needs to match the tensorflow calculation."]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dO5-OXqXjnZE","executionInfo":{"status":"ok","timestamp":1669927850625,"user_tz":300,"elapsed":163,"user":{"displayName":"Irene N. Chang","userId":"18402260844163642514"}},"outputId":"203f77c1-9932-4432-b633-92a01d268413"},"outputs":[{"output_type":"stream","name":"stdout","text":["Difference between your RNN implementation and tf RNN 5.400203e-06\n","Difference between your GRU implementation and tf GRU 4.678496e-06\n"]}],"source":["from implementation import rnn,gru\n","\n","# calculation from your own implemenation of a basic RNN\n","nprnn_outputs, nprnn_final_state = rnn(wt_h, wt_x, bias, initial_state.numpy(), input_data.numpy())\n","\n","\n","print(\"Difference between your RNN implementation and tf RNN\", \n","                     rel_error(t_rnn_outputs.numpy(), nprnn_outputs) + rel_error(t_rnn_final_state.numpy(), nprnn_final_state))\n","\n","# calculation from your own implemenation of a GRU RNN\n","npgru_outputs, npgru_final_state = gru(linear_trans_r, linear_trans_z, linear_trans_n, initial_state.numpy(), input_data.numpy())\n","\n","print(\"Difference between your GRU implementation and tf GRU\", \n","      rel_error(t_gru_outputs.numpy(), npgru_outputs) + rel_error(t_gru_final_state.numpy(), npgru_final_state))\n"]},{"cell_type":"markdown","metadata":{"id":"MIaCAFPkjnZF"},"source":["### GRU includes RNN as a special case\n","**Q4 (2 points)** Can you assign a special set of parameters to GRU such that its outputs is almost the same as RNN?"]},{"cell_type":"code","execution_count":19,"metadata":{"id":"fbwzCgxKjnZF","outputId":"46b4e9e9-75f3-480d-edf6-29bf06d8f95b","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1669927853794,"user_tz":300,"elapsed":131,"user":{"displayName":"Irene N. Chang","userId":"18402260844163642514"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Difference between RNN and a special GRU 3.2730345e-06\n"]}],"source":["# Assign some value to a parameter of GRU\n","\n","from implementation import init_gru_with_rnn\n","\n","linear_trans_r, linear_trans_z, linear_trans_n = init_gru_with_rnn(wt_h, wt_x, bias)\n","\n","# concatenate these parameters to initialize GRU kernels\n","kernel_init = np.concatenate([linear_trans_r[0], linear_trans_z[0], linear_trans_n[0]], axis=1).T\n","rec_kernel_init = np.concatenate([linear_trans_r[2], linear_trans_z[2], linear_trans_n[2]], axis=1).T\n","bias_init0 = np.concatenate([linear_trans_r[1], linear_trans_z[1], linear_trans_n[1]], axis=0)\n","bias_init1 = np.concatenate([linear_trans_r[3], linear_trans_z[3], linear_trans_n[3]])\n","\n","grurnn = nn.GRU(input_size, hidden_size, num_layers = 1, batch_first = True)\n","wt_x1, wt_h1, bias_ih1, bias_hh1 = grurnn._flat_weights\n","\n","wt_x1.data = torch.tensor(kernel_init, dtype =torch.float32)\n","wt_h1.data = torch.tensor(rec_kernel_init, dtype = torch.float32)\n","bias_ih1.data = torch.tensor(bias_init0, dtype = torch.float32)\n","bias_hh1.data = torch.tensor(bias_init1, dtype = torch.float32)\n","\n","\n","# 'outputs' is a tensor of shape [batch_size, time_steps, hidden_size]\n","# Same as the basic RNN cell, final_state == outputs[:, -1, :]\n","with torch.no_grad():\n","    t_rnn_outputs, t_rnn_final_state = t_rnn(input_data, initial_state)\n","    grurnn_outputs, grurnn_final_state = grurnn(input_data, initial_state)\n","\n","\n","# they are the same as the calculation from the basic RNN\n","print(\"Difference between RNN and a special GRU\", rel_error(t_rnn_outputs.numpy(), grurnn_outputs.numpy()))"]},{"cell_type":"markdown","metadata":{"id":"6TwbpTpyjnZG"},"source":["## Long-term dependency in GRUs\n"]},{"cell_type":"markdown","metadata":{"id":"d6VczayTjnZH"},"source":["**Q5 (2 points)** Can you set GRU parameters such that it maintains the initial state in the memory for a long term? "]},{"cell_type":"code","execution_count":20,"metadata":{"id":"UfYDyuCijnZH","outputId":"f828ea30-2665-46f5-a816-80dd8d15d0e4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1669927857886,"user_tz":300,"elapsed":104,"user":{"displayName":"Irene N. Chang","userId":"18402260844163642514"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Difference between a later hidden state and the initial state is 0.0\n"]}],"source":["from implementation import init_gru_with_long_term_memory\n","\n","\n","\n","linear_trans_r, linear_trans_z, linear_trans_n = init_gru_with_long_term_memory(input_size, hidden_size)\n","\n","# concatenate these parameters to initialize GRU kernels\n","kernel_init = np.concatenate([linear_trans_r[0], linear_trans_z[0], linear_trans_n[0]], axis=1).T\n","rec_kernel_init = np.concatenate([linear_trans_r[2], linear_trans_z[2], linear_trans_n[2]], axis=1).T\n","bias_init0 = np.concatenate([linear_trans_r[1], linear_trans_z[1], linear_trans_n[1]], axis=0)\n","bias_init1 = np.concatenate([linear_trans_r[3], linear_trans_z[3], linear_trans_n[3]])\n","\n","gru2 = nn.GRU(input_size, hidden_size, num_layers = 1, batch_first = True)\n","wt_xg, wt_hg, bias_ihg, bias_hhg = gru2._flat_weights\n","\n","\n","wt_xg.data = torch.tensor(kernel_init, dtype =torch.float32)\n","wt_hg.data = torch.tensor(rec_kernel_init, dtype = torch.float32)\n","bias_ihg.data = torch.tensor(bias_init0, dtype = torch.float32)\n","bias_hhg.data = torch.tensor(bias_init1, dtype = torch.float32)\n","\n","\n","with torch.no_grad():\n","    outputs, _ = gru2(input_data, initial_state)\n","    outputs = outputs.numpy()\n","    \n","    \n","    print('Difference between a later hidden state and the initial state is', np.mean(np.abs(outputs[:, 10, :] - initial_state[0, :, :].numpy())))\n","    "]},{"cell_type":"markdown","metadata":{"id":"r4Lft2dsjnZI"},"source":["# Implement a multi-head attention layer\n","**Q6 (5 points)** In the task, you need to implement the forward calculation of a multi-head attention layer. Your calculation needs to match the calculation of the torch MHA layer in the following test case. "]},{"cell_type":"code","execution_count":21,"metadata":{"id":"U5N4MefGjnZI","outputId":"5cdbc9a7-6703-485f-c2a5-e6321db2beb3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1669927860137,"user_tz":300,"elapsed":161,"user":{"displayName":"Irene N. Chang","userId":"18402260844163642514"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Difference between my output and torch output is  1.7749153e-08\n"]}],"source":["from rnn_param_helper import get_mha_params\n","from implementation import mha\n","\n","\n","batch_size = 4\n","time_steps = 8\n","input_size = 10\n","num_heads = 5\n","\n","input_data = torch.randn(batch_size, time_steps, input_size, dtype = torch.float32)\n","\n","\n","# run torch implementation of MHA\n","with torch.no_grad():\n","\n","    t_mha = nn.MultiheadAttention(embed_dim=input_size, num_heads=num_heads, dropout=0.0, bias=False, add_bias_kv=False, add_zero_attn=False, kdim=None, vdim=None, batch_first=True)\n","\n","    t_output, _ = t_mha(input_data, input_data, input_data, need_weights=False)\n","\n","\n","# extract model parameters from the torch MHA layer\n","Wq, Wk, Wv, Wo = get_mha_params(t_mha)\n","\n","# run the same calculation with your implementation\n","output = mha(Wq, Wk, Wv, Wo, input_data )\n","\n","print('Difference between my output and torch output is ', np.mean(np.abs(output - t_output.numpy())))\n","    \n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8l_ljgfGjnZJ"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}