{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[]}},"nbformat_minor":0,"nbformat":4,"cells":[{"cell_type":"markdown","source":["# RNN/Transformer for Modeling Sentences\n","\n","In this task, we will use an RNN or a transformer model to model sentences. The task is to predict the next character in a sentence. "],"metadata":{"id":"klUmrnwhCkjJ"}},{"cell_type":"code","source":["# As usual, a bit of setup\n","import time\n","import numpy as np\n","import torch\n","import matplotlib.pyplot as plt\n","\n","\n","%matplotlib inline\n","plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n","plt.rcParams['image.interpolation'] = 'nearest'\n","plt.rcParams['image.cmap'] = 'gray'\n","\n","# for auto-reloading external modules\n","# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n","%load_ext autoreload\n","%autoreload 2\n","%autosave 180\n"],"metadata":{"execution":{"iopub.status.busy":"2022-12-02T03:22:43.862974Z","iopub.execute_input":"2022-12-02T03:22:43.863434Z","iopub.status.idle":"2022-12-02T03:22:43.931705Z","shell.execute_reply.started":"2022-12-02T03:22:43.863389Z","shell.execute_reply":"2022-12-02T03:22:43.930728Z"},"trusted":true,"id":"JwwroA68CkjS","outputId":"43b2fc01-1ebd-43bc-b04a-b7e3ab42dd01"},"execution_count":null,"outputs":[{"name":"stdout","text":"The autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n","output_type":"stream"},{"output_type":"display_data","data":{"application/javascript":"IPython.notebook.set_autosave_interval(180000)"},"metadata":{}},{"name":"stdout","text":"Autosaving every 180 seconds\n","output_type":"stream"}]},{"cell_type":"code","source":["device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","print(device)"],"metadata":{"execution":{"iopub.status.busy":"2022-12-02T03:22:47.043175Z","iopub.execute_input":"2022-12-02T03:22:47.043642Z","iopub.status.idle":"2022-12-02T03:22:47.091740Z","shell.execute_reply.started":"2022-12-02T03:22:47.043598Z","shell.execute_reply":"2022-12-02T03:22:47.090646Z"},"trusted":true,"id":"4xgb2CEoCkjW","outputId":"94014364-5639-4855-a97c-017b52c94c82"},"execution_count":null,"outputs":[{"name":"stdout","text":"cuda:0\n","output_type":"stream"}]},{"cell_type":"markdown","source":["## Load the data\n"],"metadata":{"id":"NqeFzy-eCkjX"}},{"cell_type":"code","source":["\n","import csv\n","import string\n","import numpy as np\n","\n","def load_data(data_file):\n","    \"\"\"Load the data into a list of strings\"\"\"\n","    \n","    with open(data_file) as csv_file:\n","        reader = csv.reader(csv_file, delimiter=',')\n","        rows = list(reader)\n","\n","    if data_file == '/kaggle/input/dataset-assg4/train.csv':\n","        sentences, labels = zip(*rows[1:])\n","        sentences = list(sentences)\n","    elif data_file == '/kaggle/input/dataset-assg4/test.csv':\n","        sentences = [row[0] for row in rows[1:]]\n","    else:\n","        print(\"Can only load 'train.csv' or 'test.csv'\")\n","    \n","    # replace non ascii chars to spaces\n","    count = 0\n","    for i, sen in enumerate(sentences):\n","        count = count + sum([0 if ord(i) < 128 else 1 for i in sen])\n","        \n","        # '\\n' indicates the end of the sentence\n","        sentences[i] = ''.join([i if ord(i) < 128 else ' ' for i in sen]) + '\\n'\n","        \n","    print('The total of ', count, 'non-ascii chars are removed \\n')\n","\n","    return sentences\n","\n","def char_to_index(sentence, str_voc):\n","    \"\"\"Convert a string to an array by using the index in the vocabulary\"\"\"\n","    \n","    sen_int = np.array([str_voc.index(c) for c in sentence])\n","    return sen_int\n","\n","def convert_sen_to_data(sentences, str_voc):\n","    \"\"\" Convert a list of strings to a list of numpy arrays\"\"\"\n","    data = [None] * len(sentences)\n","    for i, sen in enumerate(sentences):\n","        data[i] = char_to_index(sen, str_voc)\n","        \n","        # sanity check\n","        #if i < 5:\n","        #    recover = \"\".join([str_voc[k] for k in data[i]])\n","        #    print(recover)\n","    return data\n","\n","\n","train_sentences = load_data('/kaggle/input/dataset-assg4/train.csv')\n","\n","# NOTE: you need to use the same vocabulary to handle your test sentences\n","vocabulary = list(set(\"\".join(train_sentences))) \n","vocabulary.sort()\n","str_voc = \"\".join(vocabulary)\n","\n","train_data = convert_sen_to_data(train_sentences, str_voc)\n","\n","\n","num_sen = len(train_data)\n","sen_lengths = [sen.shape[0] for sen in train_data]\n","max_len = max(sen_lengths)\n","min_len = min(sen_lengths)\n","num_chars = sum(sen_lengths)\n","\n","print('Data statistics:')\n","print('Number of sentences: ', num_sen)\n","print('Maximum and minimum sentence lengths:', max_len, min_len)\n","print('Total number of characters:', num_chars)\n","print('Vocabulary size: ', len(vocabulary))\n","\n","uniq, uniq_counts = np.unique(np.concatenate(train_data), return_counts=True)\n","freq = np.zeros_like(uniq_counts)\n","freq[uniq] = uniq_counts\n","\n","print('Chars in vocabulary and their frequencies:')\n","print(list(zip(vocabulary, freq.tolist())))\n","\n","\n","# a sample sentence\n","print(\"Data exploration -- showing an example sentence:\")\n","\n","sample = \"\"\n","for i in train_data[5]:\n","    sample +=str_voc[i] \n","print(sample)\n"],"metadata":{"execution":{"iopub.status.busy":"2022-12-02T03:22:49.061627Z","iopub.execute_input":"2022-12-02T03:22:49.061996Z","iopub.status.idle":"2022-12-02T03:22:55.834777Z","shell.execute_reply.started":"2022-12-02T03:22:49.061965Z","shell.execute_reply":"2022-12-02T03:22:55.833620Z"},"trusted":true,"id":"krFpmj98CkjY","outputId":"7aacacf1-ef5f-4fe3-ff04-3cc9e0238cff"},"execution_count":null,"outputs":[{"name":"stdout","text":"The total of  4328 non-ascii chars are removed \n\nData statistics:\nNumber of sentences:  160000\nMaximum and minimum sentence lengths: 100 32\nTotal number of characters: 10954565\nVocabulary size:  95\nChars in vocabulary and their frequencies:\n[('\\n', 160000), (' ', 1762678), ('!', 12100), ('#', 496), ('$', 1212), ('%', 450), ('&', 1366), (\"'\", 88729), ('(', 8734), (')', 8890), ('*', 4310), ('+', 123), (',', 33680), ('-', 20064), ('.', 108694), ('/', 1586), ('0', 11139), ('1', 10960), ('2', 7690), ('3', 3517), ('4', 2882), ('5', 4272), ('6', 2673), ('7', 2496), ('8', 2071), ('9', 2801), (':', 22223), (';', 607), ('<', 12), ('=', 103), ('>', 9), ('?', 48816), ('@', 34), ('A', 8259), ('B', 4063), ('C', 5317), ('D', 6787), ('E', 2239), ('F', 3232), ('G', 2668), ('H', 11482), ('I', 15839), ('J', 2999), ('K', 2315), ('L', 2612), ('M', 7724), ('N', 3017), ('O', 2211), ('P', 3722), ('Q', 1036), ('R', 2942), ('S', 7281), ('T', 15062), ('U', 1014), ('V', 720), ('W', 37161), ('X', 17), ('Y', 2381), ('Z', 149), ('[', 1), ('\\\\', 25), (']', 4), ('^', 322), ('_', 107), ('`', 16), ('a', 726754), ('b', 148176), ('c', 253811), ('d', 319199), ('e', 964237), ('f', 163468), ('g', 191416), ('h', 397259), ('i', 592936), ('j', 23898), ('k', 111404), ('l', 371704), ('m', 225041), ('n', 552588), ('o', 684697), ('p', 184115), ('q', 6356), ('r', 515062), ('s', 585280), ('t', 698276), ('u', 258476), ('v', 81822), ('w', 171901), ('x', 17369), ('y', 209349), ('z', 11610), ('{', 9), ('|', 66), ('}', 12), ('~', 133)]\nData exploration -- showing an example sentence:\nMartha stewart tweets hideous food photo, twitter responds accordingly\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":["## Implement an RNN or a Transformer with torch\n","\n","**Q7 (10 points)** In this problem, you are supposed to train an RNN or a transformer to model sentences. Particuarly, your model will receive 10 starting characters and should predict the rest of sentence. The model will be evaluated by per-character cross-entropy loss. You will get \n","* 5 points if your per-character cross-entropy loss is less than 2.5 (the loss by predicting with character frequencies is 3.13. Your model needs to be better than that). \n","* 8 points if your per-character cross-entropy loss is less than 2\n","* 10 points if your per-character cross-entropy loss is less than 1.5\n","\n","\\*The performance from a [paper](https://arxiv.org/pdf/1808.04444.pdf) indicates that an LSTM can achieve performance of 1.43 * ln(2) = 0.991. \n","\\*The `zip` program for compressing files roughly can achieve a performances of 3.522 bits per character. It corresponds to a performance of  3.522 * ln(2) = 2.441"],"metadata":{"id":"UcrWWxCICkja"}},{"cell_type":"code","source":["# Set up dataloader\n","\n","# TODO: please read through the code in this cell so you know the data your model will see. \n","\n","from torch.utils.data import Dataset, DataLoader\n","from torch.nn.utils.rnn import pad_sequence\n","\n","\n","class StrData(Dataset):\n","    def __init__(self, data):\n","        self.sentence = data\n","    def __len__(self):\n","        return len(self.sentence)\n","    def __getitem__(self, idx):\n","        return self.sentence[idx]\n","\n","BEGIN_ID = freq.shape[0]\n","END_ID = BEGIN_ID + 1\n","PAD_ID = BEGIN_ID + 2\n","\n","def add_begin_and_end(tokens):\n","    return torch.cat([torch.tensor([BEGIN_ID], dtype = torch.long),\n","                     torch.tensor(tokens, dtype = torch.long),\n","                     torch.tensor([END_ID], dtype = torch.long)])\n","\n","def collate_fn(batch):\n","    batch_ret = []\n","    for sentence in batch:\n","        batch_ret.append(add_begin_and_end(sentence))\n","    batch_ret = pad_sequence(batch_ret, padding_value = PAD_ID).T # pad_sequence is not batch_first\n","    return batch_ret\n","\n"],"metadata":{"execution":{"iopub.status.busy":"2022-12-02T03:23:02.027271Z","iopub.execute_input":"2022-12-02T03:23:02.027694Z","iopub.status.idle":"2022-12-02T03:23:02.064739Z","shell.execute_reply.started":"2022-12-02T03:23:02.027657Z","shell.execute_reply":"2022-12-02T03:23:02.063805Z"},"trusted":true,"id":"k7PN2PA0Ckje"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Set up a model\n","\n","Suggestion: you may want to put your model in a `.py` file. Your code might look cleaner if you do so."],"metadata":{"id":"qUJddaPYCkjg"}},{"cell_type":"code","source":["import torch.nn as nn\n","\n","# A simple model\n","class SentenceModel(nn.Module):\n","    def __init__(self):\n","        super(SentenceModel, self).__init__()\n","        self.emb = nn.Embedding(num_embeddings=freq.shape[0]+3, embedding_dim=64)\n","        self.rnn = nn.RNN(input_size=64, hidden_size = 256, num_layers=1, batch_first = True)\n","        self.linear= nn.Linear(in_features=256, out_features=freq.shape[0]+3)\n","        \n","    def forward(self, x):\n","        \n","        h = self.emb(x)\n","        h, _ = self.rnn(h)\n","        out = self.linear(h)\n","        \n","        return out\n","\n","model = SentenceModel()\n","model.to(device)"],"metadata":{"execution":{"iopub.status.busy":"2022-12-02T03:51:20.319722Z","iopub.execute_input":"2022-12-02T03:51:20.320082Z","iopub.status.idle":"2022-12-02T03:51:20.364714Z","shell.execute_reply.started":"2022-12-02T03:51:20.320051Z","shell.execute_reply":"2022-12-02T03:51:20.362818Z"},"trusted":true,"id":"4elwFZc5Ckjg","outputId":"db29a82b-a642-4c71-e088-50a4c4b4def0"},"execution_count":null,"outputs":[{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"SentenceModel(\n  (emb): Embedding(98, 32)\n  (rnn): RNN(32, 256, batch_first=True)\n  (linear): Linear(in_features=256, out_features=98, bias=True)\n)"},"metadata":{}}]},{"cell_type":"markdown","source":["### Train the model\n","\n","NOTE: this example only uses 20 sentences for fast showcase the code, but you should use the entire training set. You can also split out a subset as the validation set. You can make any changes as long as you don't touch the test set.   "],"metadata":{"id":"lufXoZGoCkjh"}},{"cell_type":"code","source":["epochs = 50\n","\n","train_loader = DataLoader(StrData(train_data), shuffle=True, batch_size = 1000, collate_fn = collate_fn)\n","\n","opt = torch.optim.Adam(model.parameters(), lr = 0.001)\n","loss_fn = torch.nn.CrossEntropyLoss(ignore_index = PAD_ID)\n","for ep in range(epochs):\n","    running_loss = 0\n","    for i, batch in enumerate(train_loader):\n","        \n","        m_input = batch[:, :-1] \n","        m_output = batch[:, 1:]\n","        ## If GPU is available, move to cuda \n","        if device.type == \"cuda\":\n","            m_input = m_input.to(device)\n","            m_output = m_output.to(device)\n","        \n","         # zero the parameter gradients\n","        opt.zero_grad()\n","        \n","        logits = model(m_input) # batch x no_sequences x logits\n","        # Question: is this teacher forcing? \n","        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), m_output.reshape(-1)) # * x vocab_size, * \n","        loss.backward()\n","        \n","        opt.step()\n","        running_loss += loss.item()\n","        \n","        # TODO: Record loss values to some variable\n","        if device.type == \"cuda\":\n","            loss = loss.cpu()\n","    print(f\"Epoch {ep+1}/{epochs}: Training Loss {running_loss / (i+1)}\")\n","    if running_loss < 1.3:\n","        break"],"metadata":{"execution":{"iopub.status.busy":"2022-12-02T03:59:31.926853Z","iopub.execute_input":"2022-12-02T03:59:31.927214Z","iopub.status.idle":"2022-12-02T04:07:02.413680Z","shell.execute_reply.started":"2022-12-02T03:59:31.927181Z","shell.execute_reply":"2022-12-02T04:07:02.412560Z"},"trusted":true,"id":"ULDmeAWQCkjj","outputId":"ee01abee-2cdf-47ec-e595-85e15eee24ec"},"execution_count":null,"outputs":[{"name":"stdout","text":"Epoch 1/50: Training Loss 2.559057368338108\nEpoch 2/50: Training Loss 2.0814728036522867\nEpoch 3/50: Training Loss 1.9154695354402065\nEpoch 4/50: Training Loss 1.8177746690809726\nEpoch 5/50: Training Loss 1.7528407923877238\nEpoch 6/50: Training Loss 1.7056243613362312\nEpoch 7/50: Training Loss 1.6695020861923695\nEpoch 8/50: Training Loss 1.6412034898996353\nEpoch 9/50: Training Loss 1.61897614300251\nEpoch 10/50: Training Loss 1.6007990851998328\nEpoch 11/50: Training Loss 1.5860715575516224\nEpoch 12/50: Training Loss 1.5736069321632384\nEpoch 13/50: Training Loss 1.5630849353969096\nEpoch 14/50: Training Loss 1.5538190700113774\nEpoch 15/50: Training Loss 1.5460318371653556\nEpoch 16/50: Training Loss 1.5388614535331726\nEpoch 17/50: Training Loss 1.5326403968036175\nEpoch 18/50: Training Loss 1.5271154433488845\nEpoch 19/50: Training Loss 1.521857362985611\nEpoch 20/50: Training Loss 1.5172783233225346\nEpoch 21/50: Training Loss 1.5131357699632644\nEpoch 22/50: Training Loss 1.5089414209127425\nEpoch 23/50: Training Loss 1.5057288624346257\nEpoch 24/50: Training Loss 1.5022329457104207\nEpoch 25/50: Training Loss 1.4993338018655777\nEpoch 26/50: Training Loss 1.4964261859655381\nEpoch 27/50: Training Loss 1.4938376359641552\nEpoch 28/50: Training Loss 1.4914177916944027\nEpoch 29/50: Training Loss 1.4892494954168796\nEpoch 30/50: Training Loss 1.4869896851480007\nEpoch 31/50: Training Loss 1.484920509904623\nEpoch 32/50: Training Loss 1.4829809091985227\nEpoch 33/50: Training Loss 1.481297417730093\nEpoch 34/50: Training Loss 1.479615868628025\nEpoch 35/50: Training Loss 1.47787661626935\nEpoch 36/50: Training Loss 1.4761705562472343\nEpoch 37/50: Training Loss 1.4750926673412323\nEpoch 38/50: Training Loss 1.4734303176403045\nEpoch 39/50: Training Loss 1.4721347562968732\nEpoch 40/50: Training Loss 1.470945493876934\nEpoch 41/50: Training Loss 1.469541223347187\nEpoch 42/50: Training Loss 1.468500120192766\nEpoch 43/50: Training Loss 1.467406839877367\nEpoch 44/50: Training Loss 1.4663665659725666\nEpoch 45/50: Training Loss 1.4651757664978504\nEpoch 46/50: Training Loss 1.4642459139227868\nEpoch 47/50: Training Loss 1.4633136466145515\nEpoch 48/50: Training Loss 1.462322573363781\nEpoch 49/50: Training Loss 1.461370261758566\nEpoch 50/50: Training Loss 1.4607421606779099\n","output_type":"stream"}]},{"cell_type":"markdown","source":["### Save the model"],"metadata":{"id":"DgwKaM6PCkjk"}},{"cell_type":"code","source":["torch.save(model, \"rnn_lm.sav\")"],"metadata":{"execution":{"iopub.status.busy":"2022-12-02T04:07:31.301621Z","iopub.execute_input":"2022-12-02T04:07:31.301976Z","iopub.status.idle":"2022-12-02T04:07:31.340660Z","shell.execute_reply.started":"2022-12-02T04:07:31.301945Z","shell.execute_reply":"2022-12-02T04:07:31.339694Z"},"trusted":true,"id":"4Ho-ybH2Ckjo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Test the trained model"],"metadata":{"id":"-PTG2OAUCkjo"}},{"cell_type":"code","source":["\n","# load the test data. NOTE: need to use the same vocabulary as the training data\n","test_sentences = load_data('/kaggle/input/dataset-assg4/test.csv')\n","test_data = convert_sen_to_data(test_sentences, str_voc)\n","\n","\n","print('Number of test instances:', len(test_data))\n","\n","# TODO: replace this stub model with your powerful model\n","model = torch.load(\"rnn_lm.sav\")\n","\n","test_dset = StrData(test_data)\n","test_loader = DataLoader(StrData(train_data), shuffle=True, batch_size = 50, collate_fn = collate_fn)\n","loss_fn = torch.nn.CrossEntropyLoss(ignore_index = PAD_ID)\n","print('Evaluating the model ...')\n","loss_sum = 0\n","char_count = 0\n","with torch.no_grad():\n","    for i, batch in enumerate(test_loader):\n","        m_input = batch[:, :-1]\n","        m_output = batch[:, 1:]\n","        if device.type == \"cuda\":\n","            m_input = m_input.to(device)\n","            m_output = m_output.to(device)\n","    \n","        logits = model(m_input)\n","        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), m_output.reshape(-1))\n","        loss_sum += loss.item()\n","        char_count += torch.sum((batch != PAD_ID) & (batch != BEGIN_ID) & (batch != END_ID))\n","        \n","per_char_loss = loss_sum / (i+1)\n","\n","print('The total number of chars in the test set is ', char_count)\n","\n","print('The per-char-loss is %.3f' % per_char_loss)\n"],"metadata":{"execution":{"iopub.status.busy":"2022-12-02T04:09:04.737442Z","iopub.execute_input":"2022-12-02T04:09:04.737922Z","iopub.status.idle":"2022-12-02T04:09:18.177839Z","shell.execute_reply.started":"2022-12-02T04:09:04.737883Z","shell.execute_reply":"2022-12-02T04:09:18.176701Z"},"trusted":true,"id":"uYA7nDhLCkjp","outputId":"433381a6-fd99-4082-a011-1963bea2e687"},"execution_count":null,"outputs":[{"name":"stdout","text":"The total of  1131 non-ascii chars are removed \n\nNumber of test instances: 40000\nEvaluating the model ...\nThe total number of chars in the test set is  tensor(10954565)\nThe per-char-loss is 1.459\n","output_type":"stream"}]},{"cell_type":"markdown","source":["### Use the model to generate sentences\n","\n","Now we can use the trained model to generate text with a starting string. The naive model just predict frequent characters in the text, so there is no meaningful generation yet. See what you get from your models."],"metadata":{"id":"ihOxjEpxCkjq"}},{"cell_type":"code","source":["import torch.distributions as distributions\n","\n","def generate_text(model, start_string, str_voc):\n","    \"\"\" Generate random text from a starting string. \"\"\"\n","\n","    # Number of characters to generate\n","    num_generate = 100 - len(start_string)\n","\n","    # Converting our start string to numbers (vectorizing)\n","    input_int = [BEGIN_ID] + [str_voc.index(s) for s in start_string]\n","    input_tensor = torch.tensor(input_int, dtype = torch.long).view([1, -1])\n","    input_tensor = input_tensor.to(device)\n","    \n","    # Empty string to store our results\n","    text_generated = []\n","\n","    # Low temperature results in more predictable text.\n","    # Higher temperature results in more surprising text.\n","    # Experiment to find the best setting.\n","    temperature = 0.5\n","    \n","    # Here batch size == 1\n","    other_voc = {BEGIN_ID: \"<BEG>\", END_ID: \"<END>\", PAD_ID: \"<PAD>\"}\n","    \n","    for i in range(num_generate):\n","        \n","        outputs = model(input_tensor)\n","        \n","        # remove the batch dimension\n","        prediction = torch.softmax(outputs[0, -1, :], dim=0)\n","\n","        # using a categorical distribution to predict the character returned by the model\n","        prediction = prediction / temperature\n","        predicted_id = int(distributions.Categorical(probs = prediction).sample())\n","\n","        \n","        # The calculation has a lot of repeatition because computation for the first part \n","        # of the sequence is the same at every iteration. But it's fine for our example.\n","        input_int.append(predicted_id)\n","        input_tensor = torch.tensor(input_int, dtype = torch.long).view([1, -1])\n","        input_tensor = input_tensor.to(device)\n","        \n","        text_generated.append(str_voc[predicted_id] if (predicted_id < len(str_voc)) else other_voc[predicted_id])\n","\n","    return (start_string + ''.join(text_generated))\n","\n","\n","start_string = 'I hav'\n","gen_sen = generate_text(model, start_string, str_voc)\n","gen_sen = gen_sen.split('\\n')[0]\n","\n","print('Starting from \"' + start_string + '\", the generated sentence is:')\n","print('\"' + gen_sen + '\"')"],"metadata":{"execution":{"iopub.status.busy":"2022-12-02T04:12:07.888434Z","iopub.execute_input":"2022-12-02T04:12:07.889282Z","iopub.status.idle":"2022-12-02T04:12:08.065232Z","shell.execute_reply.started":"2022-12-02T04:12:07.889240Z","shell.execute_reply":"2022-12-02T04:12:08.064136Z"},"trusted":true,"id":"CyEtP-HqCkjq","outputId":"78ea4e75-d529-46f7-c7bc-19a9a1e87845"},"execution_count":null,"outputs":[{"name":"stdout","text":"Starting from \"I hav\", the generated sentence is:\n\"I have to eat the man is dead incused if you hate to do in coming in american dark women.\"\n","output_type":"stream"}]},{"cell_type":"code","source":[],"metadata":{"id":"HiFKOU94Ckjr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"vN9CH9nECkjr"},"execution_count":null,"outputs":[]}]}